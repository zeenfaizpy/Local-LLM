# Local-LLM

Docker Compose for running LLM locally

Containers:

- ollama - for running models
- open-webui for chat interface


Update model name in wait_for_ollama.sh to pull at first start.
