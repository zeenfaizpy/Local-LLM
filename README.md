# Local-LLM

Docker Compose for running LLM locally

Containers:

- ollama - for running models
- open-webui for chat interface


Update model name in wait_for_ollama.sh to pull at first start.


```bash
git clone https://github.com/zeenfaizpy/Local-LLM.git
cd Local-LLM
docker compose up -d
```
